<div class="post-body">
  <div class="post-content post-summary">
    <meta name="twitter:image"
          content="https://2.bp.blogspot.com/-cY23bdQ_B_o/XXA01uQIDnI/AAAAAAAAHzs/TiSL5_tY29QMa7Sehm6VdX6cDjB6cvmrgCLcBGAs/s1600/popular%2Btimes%2B_%2Bdishes%2Bgif.gif">
    <img
      src="https://2.bp.blogspot.com/-cY23bdQ_B_o/XXA01uQIDnI/AAAAAAAAHzs/TiSL5_tY29QMa7Sehm6VdX6cDjB6cvmrgCLcBGAs/s1600/popular%2Btimes%2B_%2Bdishes%2Bgif.gif"
      style="display:none">

    <em>Posted by Miguel Guevara, Product Manager, Privacy and Data Protection Office
    </em>

    <p>
      Whether you're a city planner, a small business owner, or a software developer, gaining useful insights from data
      can help make services work better and answer important questions. But, without strong privacy protections, you
      risk losing the trust of your citizens, customers, and users.
    </p><a href="https://developers.googleblog.com/2019/09/enabling-developers-and-organizations.html" itemprop="url"
           title="Enabling developers and organizations to use differential privacy" class="read-more">Read More</a>
  </div>
  <div class="post-content post-original" itemprop="articleBody">
    <meta name="twitter:image"
          content="https://2.bp.blogspot.com/-cY23bdQ_B_o/XXA01uQIDnI/AAAAAAAAHzs/TiSL5_tY29QMa7Sehm6VdX6cDjB6cvmrgCLcBGAs/s1600/popular%2Btimes%2B_%2Bdishes%2Bgif.gif">
    <img
      src="https://2.bp.blogspot.com/-cY23bdQ_B_o/XXA01uQIDnI/AAAAAAAAHzs/TiSL5_tY29QMa7Sehm6VdX6cDjB6cvmrgCLcBGAs/s1600/popular%2Btimes%2B_%2Bdishes%2Bgif.gif"
      style="display:none">

    <em>Posted by Miguel Guevara, Product Manager, Privacy and Data Protection Office
    </em>

    <p>
      Whether you're a city planner, a small business owner, or a software developer, gaining useful insights from data
      can help make services work better and answer important questions. But, without strong privacy protections, you
      risk losing the trust of your citizens, customers, and users.
    </p>
    <p>
      Differentially-private data analysis is a principled approach that enables organizations to learn from the
      majority of their data while simultaneously ensuring that those results do not allow any individual's data to be
      distinguished or re-identified. This type of analysis can be implemented in a wide variety of ways and for many
      different purposes. For example, if you are a health researcher, you may want to compare the average amount of
      time patients remain admitted across various hospitals in order to determine if there are differences in care.
      Differential privacy is a high-assurance, analytic means of ensuring that use cases like this are addressed in a
      privacy-preserving manner.
    </p>
    <p>
      Today, we’re rolling out the open-source version of the <a href="https://github.com/google/differential-privacy/">differential
      privacy library</a> that helps power some of Google’s core products. To make the library easy for developers to
      use, we’re focusing on features that can be particularly difficult to execute from scratch, like automatically
      calculating bounds on user contributions.<strong> </strong>It is now freely available to any organization or
      developer that wants to use it.
    </p>
    <p>
      <strong>A deeper look at the technology</strong>
    </p>
    <p>
      Our open source library was designed to meet the needs of developers. In addition to being freely accessible, we
      wanted it to be easy to deploy and useful.
    </p>
    <p>
      Here are some of the key features of the library:
    </p>
    <ul>

      <li><strong>Statistical functions:</strong> Most common data science operations are supported by this release.
        Developers can compute counts, sums, averages, medians, and percentiles using our library.

      </li>
      <li><strong>Rigorous testing:</strong> Getting differential privacy right is challenging. Besides an extensive
        test suite, we’ve included an extensible ‘Stochastic Differential Privacy Model Checker library’ to help prevent
        mistakes.

      </li>
      <li><strong>Ready to use: </strong>The real utility of an open-source release is in answering the question “Can I
        use this?” That’s why we’ve included a <a href="https://www.postgresql.org/">PostgreSQL</a> extension along with
        common recipes to get you started. We’ve described the details of our approach in a <a
          href="https://arxiv.org/abs/1909.01917">technical paper</a> that we’ve just released today.

      </li>
      <li><strong>Modular:</strong> We designed the library so that it can be extended to include other functionalities
        such as additional mechanisms, aggregation functions, or privacy budget management.
      </li>
    </ul>
    <p>
      <strong>Investing in new privacy technologies</strong>
    </p>
    <p>
      We have driven the research and development of practical, differentially-private techniques since we <a
      href="https://security.googleblog.com/2014/10/learning-statistics-with-privacy-aided.html">released RAPPOR</a> to
      help improve Chrome in 2014, and continue to spearhead their real-world application.
    </p>
    <p>
      We’ve used differentially private methods to create helpful features in our products, like how busy a business is
      over the course of a day or how popular a particular restaurant’s dish is in Google Maps, and <a
      href="https://blog.google/products/project-fi/its-easier-ever-travel-project-fi/">improve Google Fi</a>.
    </p>
    <p>
      <a
        href="https://2.bp.blogspot.com/-cY23bdQ_B_o/XXA01uQIDnI/AAAAAAAAHzs/TiSL5_tY29QMa7Sehm6VdX6cDjB6cvmrgCLcBGAs/s1600/popular%2Btimes%2B_%2Bdishes%2Bgif.gif"
        imageanchor="1" target="_blank"><img alt="Screen recording on phone checking popular times of restaurant"
                                             border="0" data-original-height="1224" data-original-width="616"
                                             id="imgHalf"
                                             src="https://2.bp.blogspot.com/-cY23bdQ_B_o/XXA01uQIDnI/AAAAAAAAHzs/TiSL5_tY29QMa7Sehm6VdX6cDjB6cvmrgCLcBGAs/s1600/popular%2Btimes%2B_%2Bdishes%2Bgif.gif"></a>
    </p>
    <p>
      This year, we’ve announced several open-source, privacy technologies—<a
      href="https://medium.com/tensorflow/introducing-tensorflow-privacy-learning-with-differential-privacy-for-training-data-b143c5e801b6">Tensorflow
      Privacy</a>, <a href="https://medium.com/tensorflow/introducing-tensorflow-federated-a4147aa20041">Tensorflow
      Federated</a>, <a
      href="https://security.googleblog.com/2019/06/helping-organizations-do-more-without-collecting-more-data.html">Private
      Join and Compute</a>—and today’s launch adds to this growing list. We're excited to make this library broadly
      available and hope developers will consider leveraging it as they build out their comprehensive data privacy
      strategies. From medicine, to government, to business, and beyond, it’s our hope that these open-source tools will
      help produce insights that benefit everyone.
    </p>
    <p>
      <strong>Acknowledgements</strong>
    </p>
    <p>
      <em>Software Engineers: Alain Forget, Bryant Gipson, Celia Zhang, Damien Desfontaines, Daniel Simmons-Marengo, Ian
        Pudney, Jin Fu, Michael Daub, Priyanka Sehgal, Royce Wilson, William Lam</em>
    </p>
    <span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
  </div>
</div>
